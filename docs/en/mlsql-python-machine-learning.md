## Using Python ML Framework To Train And Predict Within MLSQL 

###  Why Integrate Python 
 You may already know that MLSQL supports [UDF/UDAF](https://github.com/allwefantasy/streamingpro/blob/master/docs/en/mlsql-script-support.md) created by Python/Scala script. 
 This brings convenient to feature engineer. Often,it is handy to have the ability to make use of Python ML Framework within MLSQL,
 as most machine learning framework provides Python language support and most ML scientists are more familiar with Python. 
 If MLSQL do not support algorithm written by Python, it will  hider its wider adoption in machine learning field.  
 
 This tutorial, we will show you how to integrated Python ML Framework.
 
## The Design
 
Python Integration is implemented by PythonAlg module which is used in train/run/predict statement.

At training stage,  PythonAlg first starts python workers according to the params provided in train/run statement,
then transport the data which already processed to them. 

There are two ways  exchanging data between JVM and Python VM.
 
1.  Serialize the data and put them into Kafka, invoke Python consumer and then feed to python processes
who execute the real training code.   

2. Serialize the data and put them to local disk where python process stand.

For Tensorflow, Kafka would be a good choice ,since most algorithm in TF is batch by batch. We can write and read in parallel.
For most other frameworks e.g. SKLearn, put the data to local disk where python processes stand would be a good option.

At predicting stage, PythonAlg will holds many python worker,  communicates with them by socket, and this is really fast so
that we can deploy as API server.  


## Usage

Say that we have already converted data to vector format, then we need to use SVM algorithm in SKLearn. You can do like 
following: 
 
```sql
 -- train sklearn model
 -- modelPath is the location where you place your model generated by sklearn.
   
 train data as PythonAlg.`${modelPath}` 
 
 -- Tell PythonAlg where the training script is
 -- Do not worry ,we will show you how to write this script 
 -- later   
 where pythonScriptPath="${sklearnTrainPath}"
 
 -- kafka params for log
 and `kafkaParam.bootstrap.servers`="${kafkaDomain}"
 
 -- distribute training data to the machine where python worker exists 
 -- PythonAlg will use json format to serialize the data and place where 
 -- training script can read
 and  enableDataLocal="true"
 and  dataLocalFormat="json"
 
 -- congigure you algorithm
 -- use SVC, and set class_weight to balanced so 
 -- sklearn will automatically fix the sample unbalanced problem
 and `fitParam.0.moduleName`="sklearn.svm"
 and `fitParam.0.className`="SVC"
 and `fitParam.0.featureCol`="features"
 and `fitParam.0.labelCol`="label"
 and `fitParam.0.class_weight`="balanced"
 and `fitParam.0.verbose`="true"

 -- python env
 and `systemParam.pythonPath`="python"
 and `systemParam.pythonParam`="-u"
 and `systemParam.pythonVer`="2.7";

```

There are many comments in the script which tell you every configuration means what.
The most import configuration is `pythonScriptPath `where you should specify the location of Python script which will do the real training.
 
Before write your own script, it would be better to know what PythonAlg can provides.
PythonAlg will tell you:
 
1. where is the data
2. Params of algorithm you have write in MLSQL script.
3. Where you should place your model once you have completed training.
 
In order to get these information, you should import mlsql module in python script.
    
```python
import mlsql
```

Get the location of data:

```python
tempDataLocalPath = mlsql.internal_system_param["tempDataLocalPath"]
```

Fetch params:

```python
def param(key, value):
    if key in mlsql.fit_param:
        res = mlsql.fit_param[key]
    else:
        res = value
    return res
    
moduleName = param("moduleName", "sklearn.svm")
className = param("className", "SVC")
```

Get the location where you should place your model:

```python
tempModelLocalPath = mlsql.internal_system_param["tempModelLocalPath"]
if not os.path.exists(tempModelLocalPath):
    os.makedirs(tempModelLocalPath)
```

These three points are the only information you should exchange with PythonAlg module. Then, be free to write anything you want.


## Training Example

Notice: sample_libsvm_data.txt is placed in  `StreamingPro HOME/streamingpro-mlsql/src/resources-local/data/mllib/`
  
```sql
load libsvm.`sample_libsvm_data.txt` as data;

train data as PythonAlg.`/tmp/model1`
where
pythonScriptPath="/tmp/train.py"

-- keep the vertion of every model you train
and keepVersion="true"

and  enableDataLocal="true"
and  dataLocalFormat="json"

and  `fitParam.0.batchSize`="1000"
and  `fitParam.0.labelSize`="2"

and validateTable="data"

and `systemParam.pythonPath`="python"
and `systemParam.pythonVer`="2.7"
and `kafkaParam.bootstrap.servers`="127.0.0.1:9092"
;
```

Here is the train.py:
 
```python
import mlsql
import os
import json
from pyspark.ml.linalg import Vectors
from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB()

tempDataLocalPath = mlsql.internal_system_param["tempDataLocalPath"]

# get training data
X = []
y = []
files = [file for file in os.listdir(tempDataLocalPath) if file.endswith(".json")]
for file in files:
    with open(tempDataLocalPath + "/" + file) as f:
        for line in f.readlines():
            obj = json.loads(line)
            f_size = obj["features"]["size"]
            f_indices = obj["features"]["indices"]
            f_values = obj["features"]["values"]
            X.append(Vectors.sparse(f_size, f_indices, f_values).toArray())
            y.append(obj["label"])

clf.partial_fit(X, y, classes=range(2))

tempModelLocalPath = mlsql.internal_system_param["tempModelLocalPath"]

if not os.path.exists(tempModelLocalPath):
    os.makedirs(tempModelLocalPath)

model_file_path = tempModelLocalPath + "/model.pkl"
print("Save model to %s" % model_file_path)
pickle.dump(model, open(model_file_path, "wb"))
```

Done. When the MLSQL script is executed, you will see the model.pkl in the sub directory of `/tmp/model1`.

We have provided a implementation of SKLearn which is more easy to use. Please check: [MLSQL-SKLearn-Example](https://github.com/allwefantasy/mlsql/tree/master/examples/sklearn)

## Predicting Example
 
Since the model have been trained , we should know how to use it.
 
```sql
load libsvm.`sample_libsvm_data.txt` as data;

-- register the model we have trained as a funciton.
register PythonAlg.`/tmp/model1` as predict options
pythonScriptPath="/tmp/predict.py"
;

-- use the predict udf
select predict(features) from data
as newdata;
```

In order to make PythonAlg know how to predict, you should provide a script like in training stage. Here is the content of the script:
 
```python
from pyspark.ml.linalg import VectorUDT, Vectors
import pickle
import python_fun

def predict(index, input):
    items = [i for i in input]
    
    ## The first element in items is vector 
    feature = VectorUDT().deserialize(pickle.loads(items[0]))
    
    ## The second element in items is the model path.
    ## To avoid loading model every time when predict, you can do like this
    modelPath = pickle.loads(items[1])[0] + "/model.pkl"
    if not hasattr(os, "mlsql_models"):
            setattr(os, "mlsql_models", {})
    if modelPath not in os.mlsql_models:
        os.mlsql_models[modelPath] = pickle.load(open(modelPath, "rb"))
    
    model = os.mlsql_models[modelPath]
    
    ## do the predicting 
    y = model.predict([feature.toArray()])
    
    ## serialize the return. The pythonAlg need the return type 
    ## is array(vector)
    return [VectorUDT().serialize(Vectors.dense(y))]

# make sure use python_fun to wrap the predict function
python_fun.udf(predict)
```

## MLFlow Support

PythonAlg module supports MLFlow project. You can get the example project [here](https://github.com/allwefantasy/streamingpro/tree/master/examples/sklearn_elasticnet_wine).

Please make sure you have conda env installed. PythonAlg will use conda to
resolve python lib dependencies according conda.yaml in project directory.

Train example:

```sql

load csv.`/Users/allwefantasy/CSDNWorkSpace/mlflow/examples/sklearn_elasticnet_wine/wine-quality.csv`
where header="true" and inferSchema="true"
as data;

train data as PythonAlg.`/tmp/abc`
 where pythonScriptPath="/Users/allwefantasy/CSDNWorkSpace/mlflow/examples/sklearn_elasticnet_wine"
 and keepVersion="true"
 and  enableDataLocal="true"
 and  dataLocalFormat="csv"
 and systemParam.envs='''{"MLFLOW_CONDA_HOME":"/anaconda3"}'''
 ;
```

Batch predict

```sql
predict data as PythonAlg.`/tmp/abc`;
```


Run StreamingPro with local with API model enabled, then register the model:

```sql
register PythonAlg.`/tmp/abc` as pj;
```

Request example:

```
import org.apache.http.client.fluent.{Form, Request}
import org.apache.spark.graphx.VertexId

object Test {
  def main(args: Array[String]): Unit = {
    val sql = "select pj(vec_dense(features)) as p1 "

    val res = Request.Post("http://127.0.0.1:9003/model/predict").bodyForm(Form.form().
      add("sql", sql).
      add("data", s"""[{"features":[ 0.045, 8.8, 1.001, 45.0, 7.0, 170.0, 0.27, 0.45, 0.36, 3.0, 20.7 ]}]""").
      add("dataType", "row")
      .build()).execute().returnContent().asString()
    println(res)
  }
}
```



## More

You can definitely integrate TensorFlow  in the same way.  

## Run  MLSQL with Tensorflow Cluster
 
The example former shows how to train with single-process. MLSQL also supports the Tensorflow cluster mode. We will show you 
how to  achieve that.
  
```sql
load libsvm.`sample_libsvm_data.txt` as data;

train data as DTFAlg.`${path}`
where
pythonScriptPath="${pythonScriptPath}"
and  keepVersion="true"

and  enableDataLocal="true"
and  dataLocalFormat="json"

and  `fitParam.0.jobName`="worker"
and  `fitParam.0.taskIndex`="0"

and  `fitParam.1.jobName`="worker"
and  `fitParam.1.taskIndex`="1"

and  `fitParam.2.jobName`="ps"
and  `fitParam.2.taskIndex`="0"

and `systemParam.pythonPath`="python"
and `systemParam.pythonVer`="2.7"
and `kafkaParam.bootstrap.servers`="127.0.0.1:9092"
;
```

Here, we have created a cluster with three nodes. 2 workers and 1 parameter server. Notice that we also should provide
a Tensorflow script.

```python
import os
import tensorflow as tf
import json
from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets

import mlsql


def param(key, value):
    if key in mlsql.fit_param:
        res = mlsql.fit_param[key]
    else:
        res = value
    return res


jobName = param("jobName", "worker")
taskIndex = int(param("taskIndex", "0"))
clusterSpec = json.loads(mlsql.internal_system_param["clusterSpec"])
checkpoint_dir = mlsql.internal_system_param["checkpointDir"]

if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)


print(mlsql.internal_system_param["clusterSpec"])
print(jobName)
print(taskIndex)


def model(images):
    """Define a simple mnist classifier"""
    net = tf.layers.dense(images, 500, activation=tf.nn.relu)
    net = tf.layers.dense(net, 500, activation=tf.nn.relu)
    net = tf.layers.dense(net, 10, activation=None)
    return net


def run():
    # create the cluster configured by `ps_hosts' and 'worker_hosts'
    cluster = tf.train.ClusterSpec(clusterSpec)

    # create a server for local task
    server = tf.train.Server(cluster, job_name=jobName,
                             task_index=taskIndex)

    if jobName == "ps":
        server.join()  # ps hosts only join
    elif jobName == "worker":
        # workers perform the operation
        # ps_strategy = tf.contrib.training.GreedyLoadBalancingStrategy(FLAGS.num_ps)

        # Note: tf.train.replica_device_setter automatically place the paramters (Variables)
        # on the ps hosts (default placement strategy:  round-robin over all ps hosts, and also
        # place multi copies of operations to each worker host
        with tf.device(tf.train.replica_device_setter(worker_device="/job:worker/task:%d" % (taskIndex),
                                                      cluster=cluster)):
            # load mnist dataset
            mnist = read_data_sets("./dataset", one_hot=True)

            # the model
            images = tf.placeholder(tf.float32, [None, 784])
            labels = tf.placeholder(tf.int32, [None, 10])

            logits = model(images)
            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))

            # The StopAtStepHook handles stopping after running given steps.
            hooks = [tf.train.StopAtStepHook(last_step=2000)]

            global_step = tf.train.get_or_create_global_step()
            optimizer = tf.train.AdamOptimizer(learning_rate=1e-04)

            if True:
                # asynchronous training
                # use tf.train.SyncReplicasOptimizer wrap optimizer
                # ref: https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer
                optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=2,
                                                           total_num_replicas=2)
                # create the hook which handles initialization and queues
                hooks.append(optimizer.make_session_run_hook((taskIndex == 0)))

            train_op = optimizer.minimize(loss, global_step=global_step,
                                          aggregation_method=tf.AggregationMethod.ADD_N)

            # The MonitoredTrainingSession takes care of session initialization,
            # restoring from a checkpoint, saving to a checkpoint, and closing when done
            # or an error occurs.
            with tf.train.MonitoredTrainingSession(master=server.target,
                                                   is_chief=(taskIndex == 0),
                                                   checkpoint_dir=checkpoint_dir,
                                                   hooks=hooks) as mon_sess:

                while not mon_sess.should_stop():
                    # mon_sess.run handles AbortedError in case of preempted PS.
                    img_batch, label_batch = mnist.train.next_batch(32)
                    _, ls, step = mon_sess.run([train_op, loss, global_step],
                                               feed_dict={images: img_batch, labels: label_batch})
                    if step % 100 == 0:
                        print("Train step %d, loss: %f" % (step, ls))

run()

```









 